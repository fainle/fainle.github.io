<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Fainle的博客">
    <meta name="keyword"  content="github">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        一：线性回归 (Linear Regression) - Fainle的博客 | Fainle&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 人生苦短，多思考，少编程 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/avatar.jpg" />
        </div>
        <div class="name">
            <i>Fainle</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archive">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#一-概述"><span class="toc-text">一 . 概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二-所属分类"><span class="toc-text">二 . 所属分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三-数学相关"><span class="toc-text">三 . 数学相关</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Mean-Absolute-Error-平均绝对误差"><span class="toc-text">1. Mean Absolute Error (平均绝对误差)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Mean-Squared-Error-均方误差"><span class="toc-text">2. Mean Squared Error (均方误差)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#四-梯度下降示例代码"><span class="toc-text">四 . 梯度下降示例代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#五-线性回归示例代码-sklearn"><span class="toc-text">五 . 线性回归示例代码 (sklearn)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#六-多元线性回归"><span class="toc-text">六 . 多元线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一些问题"><span class="toc-text">一些问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多项式回归"><span class="toc-text">多项式回归</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 人生苦短，多思考，少编程 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        一：线性回归 (Linear Regression)
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2020-07-21 00:00:00</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#机器学习笔记" title="机器学习笔记">机器学习笔记</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h3 id="一-概述"><a href="#一-概述" class="headerlink" title="一 . 概述"></a>一 . 概述</h3><p>在统计学中，线性回归是利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系的一种分析。</p>
<p>一个自变量称为一元回归。</p>
<p>多个自变量称为多元回归。</p>
<h3 id="二-所属分类"><a href="#二-所属分类" class="headerlink" title="二 . 所属分类"></a>二 . 所属分类</h3><p>线性回归一般属于 监督学习。</p>
<h3 id="三-数学相关"><a href="#三-数学相关" class="headerlink" title="三 . 数学相关"></a>三 . 数学相关</h3><p>一般用梯度下降的方法来拟合线性回归的线，让其尽量的均匀分割散列点。</p>
<p>其中以下是常用的损失函数，用于计算最小化损失。</p>
<h4 id="1-Mean-Absolute-Error-平均绝对误差"><a href="#1-Mean-Absolute-Error-平均绝对误差" class="headerlink" title="1. Mean Absolute Error (平均绝对误差)"></a>1. Mean Absolute Error (平均绝对误差)</h4><script type="math/tex; mode=display">Error = \frac{1}{m}\sum_{i=1}^M|y-\hat{y}|</script><p><code>缺点: 容易造成收敛缓慢或不收敛</code></p>
<h4 id="2-Mean-Squared-Error-均方误差"><a href="#2-Mean-Squared-Error-均方误差" class="headerlink" title="2. Mean Squared Error (均方误差)"></a>2. Mean Squared Error (均方误差)</h4><script type="math/tex; mode=display">Error = \frac{1}{2m}\sum_{i=1}^M(y-\hat{y})^2</script><p><code>缺点: 容易受到较大异常值干扰而使斜率偏向较大异常值造成欠拟合</code></p>
<h3 id="四-梯度下降示例代码"><a href="#四-梯度下降示例代码" class="headerlink" title="四 . 梯度下降示例代码"></a>四 . 梯度下降示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data = np.loadtxt(<span class="string">'data.csv'</span>, delimiter=<span class="string">','</span>)  <span class="comment"># 加载数据</span></span><br><span class="line">X = data[:, :<span class="number">-1</span>]  <span class="comment"># 变量</span></span><br><span class="line">y = data[:, <span class="number">-1</span>]  <span class="comment"># 值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制数据点 用于观察一下大致分布</span></span><br><span class="line">plt.scatter(X, y, marker=<span class="string">'.'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换数据集</span></span><br><span class="line">data = np.hstack((np.ones((data.shape[<span class="number">0</span>], <span class="number">1</span>)), data))</span><br><span class="line">X_train = data[:, :<span class="number">-1</span>]</span><br><span class="line">y_train = data[:, <span class="number">-1</span>].reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hypothesis</span><span class="params">(X, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    进行预测</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(X, theta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># function to compute gradient of error function w.r.t. theta</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h = hypothesis(X, theta)</span><br><span class="line">    grad = np.dot(X.transpose(), (h - y))</span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算损失函数值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h = hypothesis(X, theta)</span><br><span class="line">    J = <span class="number">1</span> / <span class="number">2</span> * np.dot((h - y).transpose(), (h - y))</span><br><span class="line">    <span class="keyword">return</span> J[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, y, learning_rate=<span class="number">0.001</span>, batch_size=<span class="number">25</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    梯度下降算法</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    history_cost = []</span><br><span class="line">    theta = np.zeros((X.shape[<span class="number">1</span>], <span class="number">1</span>))</span><br><span class="line">    n_points = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        batch = np.random.choice(range(n_points), batch_size)</span><br><span class="line"></span><br><span class="line">        X_batch = X[batch, :]</span><br><span class="line">        y_batch = y[batch]</span><br><span class="line"></span><br><span class="line">        theta = theta - learning_rate * gradient(X_batch, y_batch, theta)</span><br><span class="line">        history_cost.append(cost(X_batch, y_batch, theta))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, history_cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta, error_list = gradient_descent(X_train, y_train, batch_size=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Bias = "</span>, theta[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"Coefficients = "</span>, theta[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># visualising gradient descent</span></span><br><span class="line">plt.plot(error_list)</span><br><span class="line">plt.xlabel(<span class="string">"Number of iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Cost"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">y_pred = hypothesis(X_train, theta)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">plt.scatter(X, y, marker=<span class="string">'.'</span>)</span><br><span class="line">plt.plot(X_train[:, <span class="number">1</span>], y_pred, color=<span class="string">'orange'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="五-线性回归示例代码-sklearn"><a href="#五-线性回归示例代码-sklearn" class="headerlink" title="五 . 线性回归示例代码 (sklearn)"></a>五 . 线性回归示例代码 (sklearn)</h3><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="title">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">data</span> = pd.read_csv('<span class="title">data</span>.<span class="title">csv'</span>, <span class="title">delimiter</span> = ',')  # 加载数据</span></span><br><span class="line"></span><br><span class="line"><span class="type">X</span> = <span class="class"><span class="keyword">data</span>.iloc[:,0].to_frame()</span></span><br><span class="line"><span class="title">y</span> = <span class="class"><span class="keyword">data</span>.iloc[:,1].to_frame()</span></span><br><span class="line"></span><br><span class="line"><span class="title">lr_model</span> = <span class="type">LinearRegression</span>()</span><br><span class="line"><span class="title">reg</span> = lr_model.fit(<span class="type">X</span>, y)</span><br><span class="line"><span class="title">y_pred</span> = reg.predict(<span class="type">X</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">plt</span>.scatter(<span class="type">X</span>, y, marker='.')</span><br><span class="line"><span class="title">plt</span>.plot(<span class="type">X</span>, y_pred, color='orange')</span><br><span class="line"><span class="title">plt</span>.show()</span><br></pre></td></tr></table></figure>
<h3 id="六-多元线性回归"><a href="#六-多元线性回归" class="headerlink" title="六 . 多元线性回归"></a>六 . 多元线性回归</h3><p>多元线性回归基本特征和单元一直，但多元线性回归无法可视化。</p>
<p>多元线性回归例子</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line"></span><br><span class="line">boston_data = load_boston()</span><br><span class="line">x = boston_data['data']</span><br><span class="line">y = boston_data['target']</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line">reg = model.fit(x, y)</span><br><span class="line"></span><br><span class="line">sample_house = [[<span class="number">2</span>.<span class="number">29690000</span>e-<span class="number">01</span>, <span class="number">0</span>.<span class="number">00000000</span>e+<span class="number">00</span>, <span class="number">1</span>.<span class="number">05900000</span>e+<span class="number">01</span>, <span class="number">0</span>.<span class="number">00000000</span>e+<span class="number">00</span>, <span class="number">4</span>.<span class="number">89000000</span>e-<span class="number">01</span>,</span><br><span class="line">                <span class="number">6</span>.<span class="number">32600000</span>e+<span class="number">00</span>, <span class="number">5</span>.<span class="number">25000000</span>e+<span class="number">01</span>, <span class="number">4</span>.<span class="number">35490000</span>e+<span class="number">00</span>, <span class="number">4</span>.<span class="number">00000000</span>e+<span class="number">00</span>, <span class="number">2</span>.<span class="number">77000000</span>e+<span class="number">02</span>,</span><br><span class="line">                <span class="number">1</span>.<span class="number">86000000</span>e+<span class="number">01</span>, <span class="number">3</span>.<span class="number">94870000</span>e+<span class="number">02</span>, <span class="number">1</span>.<span class="number">09700000</span>e+<span class="number">01</span>]]</span><br><span class="line"></span><br><span class="line">reg.predict(sample_house)</span><br></pre></td></tr></table></figure>
<h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><p>1 数据分布非线形的时候 线性回归不理想</p>
<p>2 数据异常值较大时 会引起欠拟合</p>
<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">from sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">from sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(<span class="string">'data1.csv'</span>)</span><br><span class="line">X = train_data[<span class="string">'Var_X'</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = train_data[<span class="string">'Var_Y'</span>].values</span><br><span class="line"></span><br><span class="line">poly_feat = PolynomialFeatures(degree = <span class="number">2</span>)</span><br><span class="line">X_poly = poly_feat.fit_transform(X)</span><br><span class="line"></span><br><span class="line">poly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)</span><br><span class="line"></span><br><span class="line">y_pred = poly_model.predict(X_poly)</span><br><span class="line"></span><br><span class="line">plt.scatter(X, y, marker=<span class="string">'.'</span>)</span><br><span class="line">plt.plot(X, y_pred, <span class="built_in">color</span>=<span class="string">'blue'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        
        <li>
            <a target="_blank"  href="https://github.com/Fainle">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        

    </ul>
    
    <p>
        <span>/</span>
        
    </p>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>

<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
