[{"title":"三：Softmax 函数","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：三 Softmax 函数/","content":"\n### 一 . 概述\n\n它能将一个含任意实数的K维向量 `z` “压缩”到另一个K维实向量 $$ \\sigma (z) $$中，使得每一个元素的范围都在(0, 1)之间，并且所有元素的和为1(也可视为一个 (k-1)维的hyperplan，因为总和为1，所以是subspace)。\n\n$$ \n\\sigma(z)_{j} = \\frac{e^{zj}}{\\sum_{k=1}^{K}e^{z}k} \\quad for\\ j = 1 ... k\n$$\n\n\n\n### 二 . 实现代码\n\n```python\nimport numpy as np\n\nl = np.array([-1, 0, 1])\n\ndef softmax(l):\n    exp = np.exp(l)\n    return np.divide(exp, exp.sum())\n\nsoftmax(l)\n```","tags":["深度学习笔记"]},{"title":"二：Sigmoid 函数","url":"/2020/07/27/machine learning/Neural Networks/神经网络学习笔记：二 sigmoid 函数/","content":"\n### 一 . 概述\n\nSigmoid函数得名因其形状像S字母。一种常见的S函数是逻辑函数\n\n$$ \nS(t) = \\frac{1}{1 + e^{-t}}\n$$\n\n优点：平滑、易于求导。\n\n缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。\n","tags":["深度学习笔记"]},{"title":"一：感知器 (Perceptrons)","url":"/2020/07/27/machine learning/Neural Networks/神经网络学习笔记：一 感知器/","content":"\n### 一 . 概述\n\n感知器是一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。\n\n### 二 . 定义\n\n感知器使用特征向量来表示的前馈神经网络。\n\n$$ \nf(x) = \n\\begin{cases} \n1\\quad if w.x + b > 0 \\\\ \n0\\quad else\n\\end{cases} \n\n$$\n\nw是实数的表示权重的向量，`w.x`是点积。`b`是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。\n\n### 三 .作为逻辑运算符的感知器 \n\n这种感知器 可以执行 `and` `or` 和 `not` 运算。\n\n这种感知器有2个输入值为{true, false} 类似 {1, 0} 和 一个输出 `yes` or `no`.\n\n#### 1 . AND 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  yes  |\n| 1  | 0 |  no  |\n| 0  | 1 |  no  |\n| 0  | 0 |  no  |\n\n#### 2 . OR 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  yes  |\n| 1  | 0 |  yes  |\n| 0  | 1 |  yes  |\n| 0  | 0 |  no  |\n\n#### 3 . NOT 感知器\n\n|   IN   |  OUT |\n|  ----  |  ---- |\n| 1  |  yes  |\n| 0   |  no  |\n\n#### 4 . XOR 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  no  |\n| 1  | 0 |  yes  |\n| 0  | 1 |  yes |\n| 0  | 0 |  no |\n\n\n### 四 . 感知器算法\n\n```\nimport pandas as pd\n\nnp.random.seed(42)\n\ndef stepFunction(t):\n    if t >= 0:\n        return 1\n    return 0\n\n## 设置函数\ndef prediction(X, W, b):\n    return stepFunction((np.matmul(X,W)+b)[0])\n\n## 计算新权重\ndef perceptronStep(X, y, W, b, learn_rate = 0.01):\n    for i in range(len(X)):\n        y_hat = prediction(X[i],W,b)\n        \n        if y[i] - y_hat == 1:\n            W[0] += X[i][0]*learn_rate\n            W[1] += X[i][1]*learn_rate\n        elif y[i] - y_hat == -1:\n            W[0] -= X[i][0]*learn_rate\n            W[1] -= X[i][1]*learn_rate\n    \n    return W, b\n    \n## 感知器算法\ndef trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n    x_min, x_max = min(X.T[0]), max(X.T[0])\n    y_min, y_max = min(X.T[1]), max(X.T[1])\n    W = np.array(np.random.rand(2,1))\n    b = np.random.rand(1)[0] + x_max\n    # These are the solution lines that get plotted below.\n    boundary_lines = []\n    for i in range(num_epochs):\n        # In each epoch, we apply the perceptron step.\n        W, b = perceptronStep(X, y, W, b, learn_rate)\n        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n    return boundary_lines\n\n\ndata = np.asarray(pd.read_csv('data.csv'))\nx = data[:, 0:data.shape[1]-1]\ny = data[:, -1]\n\nres = trainPerceptronAlgorithm(x, y)\n```\n\n","tags":["深度学习笔记"]},{"title":"七：独热编码 (One-Hot Encoding)","url":"/2020/07/27/machine learning/机器学习笔记七：文本独热编码/","content":"\n### 一 . 概述\n\n使用pandas对Dataframe上进行One-Hot Encoding的例子。\n\n### 二 . Pandas Dataframe 中的热编码。\n\n首先我们创建一个DataFrame数据集。\n\n```\nimport pandas as pd\n\ndata = pd.DataFrame({'color': ['blue', 'red', 'yellow']})\n```\n\n![](/img/1.png)\n\npandas上有一个`get_dummies`方法可以对DataFrame数据，实现我们想要的独热编码功能。\n\n```\npd.get_dummies(data['color'], prefix='color')  ## 其中 prefix 为新的特征名称前缀\n```\n\n![](/img/2.png)\n\n有时候我们可能会出现在预测数据集合里面存在 在训练集合里面没有的特质数据。比如 `green`，这样会让独热编码出错。这种情况可以使用CategoricalDType方法来处理\n\n```\ndf['color'] = df['color'].astype(pd.CategoricalDtype(['blue', 'green']))\n```\n\n这样其它异常值会被忽略\n\n使用pd.concat可以将新的数据列和原始数据链接起来\n\n```\ndf = pd.concat([df, pd.get_dummies(df['color'], prefix='color')], axis=1)\n```\n\n也可以使用 `dummy_na=True` 参数保留 `NaN` 值为一个单独的独热编码\n\n```\ndf = pd.concat([df, pd.get_dummies(df['color'], prefix='color', dummy_na=True)], axis=1)\n```\n\n最后结构如下图\n\n![](/img/3.png)\n\n","tags":["pandas"]},{"title":"六：模型评估 (Model Evaluation Metrics)","url":"/2020/07/25/machine learning/机器学习笔记六：模型评估/","content":"\n### 一 . 概述\n\n模型评估是通过一系列评估方法评估模型的好坏。\n\n### 二 . Confusion Matrix (混淆矩阵)\n\n在机器学习领域和统计分类问题中，混淆矩阵是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。之所以如此命名，是因为通过这个矩阵可以方便地看出机器是否将两个不同的类混淆了。\n\n|     | 真  |  假 |\n|  ----  | ----  |  ---- |\n| 真  | 真真 = true |  真假 = false  |\n| 假  | 假真 = false |  假假 = true  |\n\n### 三 . Accuracy (准确度)\n\n$$ acc = \\frac{真真 + 假假}{总数} $$ \n\n\n### 四 . Precision （准确率)\n\n$$ precision = \\frac{真真}{真真 + 假真} $$ \n\n查得更准\n\n### 五 . Recall （召回率)\n\n$$ recall = \\frac{真真}{真真 + 真假 } $$ \n\n查得更全\n\n一般情况下 准确率 和 召回率 越高越好，但真实情况需要根据业务的情况。\n\n### 六 . F1 Score \n\n$$ f1 = 2\\frac{precision * recall}{precision + recall } $$ \n\n更偏向于那个指标。\n\n### 七 . ROC \n\nroc 越大模型越好\n\n### 八 . sklearn 例子\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve\nimport numpy as np\n\ndata = np.asarray(pd.read_csv('data.csv'))\nx = data[:, 0:2]\ny = data[:, 2]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nprecision_score = precision_score(y_test, y_pred)\nrecall_score = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\nroc_curve = roc_curve(y_test, y_pred)\n\nprint('acc: ', acc)\nprint('precision_score: ', precision_score)\nprint('recall_score: ', recall_score)\nprint('f1_score: ', f1_score)\nprint('roc_curve: ', roc_curve)\n```\n","tags":["统计机器学习笔记"]},{"title":"五：集成方法 (Ensemble Methods)","url":"/2020/07/25/machine learning/机器学习笔记五：集成方法/","content":"\n### 一 . 概述\n\n集成方法是将多个模型组合起来获得更好的模型的一种方法。一般分为 bagging 和 boosting。\n\n### 二 . 所属分类\n\n集成方法属于 监督学习。\n  \n### 三 . bagging\n\n装袋算法, 对多个模型的结果投票，平均等方式获得结果。\n\n代表算法:\n\n#### 1 . 随机森林 (Random Forests)\n\n随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。\n\n### 四 . boosting。\n\n提升方法\n\n多个弱分类器组合成一个强分类器的算法，通常给不同的弱分类器不同的权重。数据会被重新加权。\n\n代表算法:\n\n#### 1 . AdaBoost\n\n自适应增强\n\nAdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。\n\n其中权重计算公式 \n\n$$ weight = ln(\\frac{accuracy}{1-accuracy}) $$\n\n例子\n```\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), n_estimators = 4) # base_estimator 选择的弱模型，n_estimators 模型的数量\n\nmodel.fit(x_train, y_train)\nmodel.predict(x_test)\n```","tags":["统计机器学习笔记"]},{"title":"四：支持向量机 (Support Vector Machine)","url":"/2020/07/24/machine learning/机器学习笔记四：支持向量机/","content":"\n### 一 . 概述\n\n支持向量机是一种分类算法，它可以找到潜在的最佳分割线。\n\n### 二 . 所属分类\n\n支持向量机属于 监督学习。\n  \n### 三 . Kernels\n\n#### 1 . polynomial\n\n#### 2 . rbf\n\n### 四 . sklearn 支持向量机示例\n\n```\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\n\ndata = np.asarray(pd.read_csv('data.csv', header=None))\n\nX = data[:,0:2]\ny = data[:,2]\n\nmodel = SVC(kernel='rbf', gamma=27)\n\nmodel.fit(X,y)\n\ny_pred = model.predict(X)\n\nacc = accuracy_score(y, y_pred)\n```","tags":["统计机器学习笔记"]},{"title":"一：朴素贝叶斯垃圾邮件分类","url":"/2020/07/24/machine learning/机器学习练习项目一：垃圾邮件分类/","content":"\n### 一 . 概述\n\n使用朴素贝叶斯算法实现垃圾邮件分类。\n\n###二 . 数据集\n\n[kaggle提供的数据集](https://www.kaggle.com/venky73/spam-mails-dataset)\n  \n### 三 . 代码\n\n```python\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # td-if\nfrom sklearn.model_selection import train_test_split # 用于切割数据\nfrom sklearn.naive_bayes import MultinomialNB # 朴素贝叶斯模型\n\ndata = pd.read_csv('spam_ham_dataset.csv') # 载入数据\n\nx = data[['text']] # 特征\ny = data[['label']] # 值\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # 划分训练数据和测试数据 比例 1:4\n\nprint('train data number: {d}'.format(d=len(x_train)))\nprint('test data number: {d}'.format(d=len(x_test)))\n\ncv = CountVectorizer()\ncv.fit(x['text'])\ncount = cv.transform(x_train['text'])\n\ntfidf = TfidfTransformer()\ntfidf.fit(count)\ntfidf_matrix = tfidf.transform(count)\n\nmodel = MultinomialNB()\nmodel.fit(tfidf_matrix, y_train)\n\nmodel.predict(tfidf.transform(cv.transform(x_test['text'])))\n\nmodel.score(tfidf.transform(cv.transform(x_test['text'])), y_test)\n\n```\n","tags":["机器学习练习项目"]},{"title":"三：朴素贝叶斯 (Naive Bayes)","url":"/2020/07/23/machine learning/机器学习笔记三：朴素贝叶斯/","content":"\n### 一 . 概述\n\n朴素贝叶斯是一种概率算法，基于条件概率这样概念。它具有容易实现，训练速度很快等优点。\n\n###二 . 所属分类\n\n朴素贝叶斯属于 监督学习。\n  \n### 三 . 贝叶斯推理公式\n\n$$ P(A|R) = \\frac{P(A)P(R|A)}{P(A)P(R|A) + P(B)P(R|B)} $$\n\n`其中 P(A) P(B) 为先验概率`\n\n### 四 . 练习题\n\n已知一种传染病的感染概率为万分之一，医生检查准确率为99%，如果一个病人结果为阳性，请问他得这种传染病的概率是多少？\n\nA = 感染 B = 健康 R = 后验概率\n\n答案\n\n>P(A) = 0.0001 (得病概率)\n> P(B) = 0.9999 (健康概率)\n> P(R|A) = 0.99 (后验得病概率)\n> p(R|B) = 0.01 (后验健康概率)\n> P(A|R) = 0.0001 * 0.99 / (0.0001 * 0.99 + 0.9999 * 0.01) = 0.0098\n\n### 五 . 贝叶斯算法\n\n#### 1 . Gaussian Naive Bayes\n#### 2 . Multinomial Naive Bayes\n#### 3 . Complement Naive Bayes\n#### 4 . Bernoulli Naive Bayes\n#### 5 . Categorical Naive Bayes\n#### 6 . Out-of-core naive Bayes model fitting\n\n### 六 . sklearn 多项式朴素贝叶斯（Multinomial Naive Bayes）示例\n\n```python\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\n\nrng = np.random.RandomState(1)\n\nX = rng.randint(10, size=(2, 100))\ny = np.array([1, 2])\n\nclf.fit(X, y)\n\nyred = clf.predict(X[0:1])\n```","tags":["统计机器学习笔记"]},{"title":"二：决策树 (Decision Trees)","url":"/2020/07/23/machine learning/机器学习笔记二：决策树/","content":"\n### 一 . 概述\n\n决策树是一个预测模型；它代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，可以用于分析数据，同样也可以用来作预测。\n\n###二 . 所属分类\n\n决策树一般属于 监督学习。\n  \n### 三 . Entropy (熵)\n\n决策树需要用到熵。\n\n熵是一个物理学概念，是一种测量在动力学方面不能做功的能量总数，也就是当总体的熵增加，其做功能力也下降，熵的量度正是能量退化的指标。熵亦被用于计算一个系统中的失序现象，也就是计算该系统混乱的程度。熵越低越稳定。\n\n公式\n\n$$ Entropy = -p_1log_2(p_1) - p_2log_2(p_2) ... -p_nlog_2(p_n) $$\n\n简化\n$$ Entropy = -\\sum_{n=1}^np_nlog_2(p_n) $$\n\n其中p \n$$ p_i = \\frac{p_i}{n} $$ \n\n\n### 四 . Information Gain (信息增益)\n\n信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度。\n\n信息增益是特征选择的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，相应的信息增益也就越大。\n\n$$ Gain(D) = Entropy(D) - \\frac{D^v}{D}\\sum_{v=1}^vEntropy(D^v) $$\n\n### 五 . sklearn 决策树示例\n\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata = np.asarray(pd.read_csv('data.csv', header=None)) # 读取数据 去掉header头 并转为numpy array\n\nX = data[:, 0:2]\ny= data[:, 2]\n\nmodel = DecisionTreeClassifier(max_depth=7, min_samples_leaf=1, ) # 选择决策树模型 其中最大深度为7 最小样本数为10 （当前数据中 如最小样本数为1 则可以准确性达到 100% , 最小样本数决定是否剪枝)\n\nreg = model.fit(X,y)  # 训练模型\ny_pred = model.predict(X)  #预测模型\n\nacc = accuracy_score(y, y_pred)  # 查看计算准确度\n```\n\n### 六 . 可视化\n```\nimport graphviz\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(model, out_file='tree.dot',\n#                                 feature_names=data.columns[:-1], # 特征名称\n                                class_names=['No', 'Yes'], # 目标变量的类别名\n                                filled=True, rounded=True,\n                                special_characters=True)\n\nwith open('tree.dot') as f:\n    dot = f.read()\n    \ngraphviz.Source(dot)\n```\n\n","tags":["统计机器学习笔记"]},{"title":"一：线性回归 (Linear Regression)","url":"/2020/07/21/machine learning/机器学习笔记一：线性回归/","content":"\n### 一 . 概述\n在统计学中，线性回归是利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系的一种分析。\n\n一个自变量称为一元回归。\n\n多个自变量称为多元回归。\n\n###二 . 所属分类\n\n线性回归一般属于 监督学习。\n  \n### 三 . 数学相关\n\n一般用梯度下降的方法来拟合线性回归的线，让其尽量的均匀分割散列点。\n\n其中以下是常用的损失函数，用于计算最小化损失。\n\n#### 1. Mean Absolute Error (平均绝对误差)\n\n$$ Error = \\frac{1}{m}\\sum_{i=1}^M|y-\\hat{y}| $$\n\n`缺点: 容易造成收敛缓慢或不收敛`\n\n#### 2. Mean Squared Error (均方误差)\n\n$$ Error = \\frac{1}{2m}\\sum_{i=1}^M(y-\\hat{y})^2 $$\n\n`缺点: 容易受到较大异常值干扰而使斜率偏向较大异常值造成欠拟合`\n\n### 四 . 梯度下降示例代码\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt('data.csv', delimiter=',')  # 加载数据\nX = data[:, :-1]  # 变量\ny = data[:, -1]  # 值\n\n\n# 绘制数据点 用于观察一下大致分布\nplt.scatter(X, y, marker='.')\nplt.show()\n\n# 转换数据集\ndata = np.hstack((np.ones((data.shape[0], 1)), data))\nX_train = data[:, :-1]\ny_train = data[:, -1].reshape((-1, 1))\n\n\ndef hypothesis(X, theta):\n    \"\"\"\n    进行预测\n    \"\"\"\n    return np.dot(X, theta)\n\n\n# function to compute gradient of error function w.r.t. theta\ndef gradient(X, y, theta):\n    \"\"\"\n    计算梯度\n    \"\"\"\n    h = hypothesis(X, theta)\n    grad = np.dot(X.transpose(), (h - y))\n    return grad\n\n\ndef cost(X, y, theta):\n    \"\"\"\n    计算损失函数值\n    \"\"\"\n    h = hypothesis(X, theta)\n    J = 1 / 2 * np.dot((h - y).transpose(), (h - y))\n    return J[0]\n\n\ndef gradient_descent(X, y, learning_rate=0.001, batch_size=25):\n    \"\"\"\n    梯度下降算法\n    \"\"\"\n    history_cost = []\n    theta = np.zeros((X.shape[1], 1))\n    n_points = X.shape[0]\n\n    for _ in range(batch_size):\n        batch = np.random.choice(range(n_points), batch_size)\n\n        X_batch = X[batch, :]\n        y_batch = y[batch]\n\n        theta = theta - learning_rate * gradient(X_batch, y_batch, theta)\n        history_cost.append(cost(X_batch, y_batch, theta))\n\n    return theta, history_cost\n\n\ntheta, error_list = gradient_descent(X_train, y_train, batch_size=1000)\nprint(\"Bias = \", theta[0])\nprint(\"Coefficients = \", theta[1:])\n\n# visualising gradient descent\nplt.plot(error_list)\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\n\ny_pred = hypothesis(X_train, theta)\n#\nplt.scatter(X, y, marker='.')\nplt.plot(X_train[:, 1], y_pred, color='orange')\nplt.show()\n\n```\n\n### 五 . 线性回归示例代码 (sklearn)\n\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv('data.csv', delimiter = ',')  # 加载数据\n\nX = data.iloc[:,0].to_frame()\ny = data.iloc[:,1].to_frame()\n\nlr_model = LinearRegression()\nreg = lr_model.fit(X, y)\ny_pred = reg.predict(X)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='orange')\nplt.show()\n```\n\n### 六 . 多元线性回归\n\n多元线性回归基本特征和单元一直，但多元线性回归无法可视化。\n\n多元线性回归例子\n\n```\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\n\nmodel = LinearRegression()\n\nreg = model.fit(x, y)\n\nsample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n\nreg.predict(sample_house)\n```\n\n### 一些问题\n\n1 数据分布非线形的时候 线性回归不理想\n\n2 数据异常值较大时 会引起欠拟合\n\n\n### 多项式回归\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_data = pd.read_csv('data1.csv')\nX = train_data['Var_X'].values.reshape(-1, 1)\ny = train_data['Var_Y'].values\n\npoly_feat = PolynomialFeatures(degree = 2)\nX_poly = poly_feat.fit_transform(X)\n\npoly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)\n\ny_pred = poly_model.predict(X_poly)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='blue')\nplt.show()\n```\n\n\n","tags":["统计机器学习笔记"]}]