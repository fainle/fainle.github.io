[{"title":"一：线性回归","url":"/2020/07/21/机器学习笔记一：线性回归/","content":"\n### 一 . 概述\n在统计学中，线性回归是利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系的一种分析。\n\n一个自变量称为一元回归。\n\n多个自变量称为多元回归。\n\n###二 . 所属分类\n\n线性回归一般属于 监督学习。\n  \n### 三 . 数学相关\n\n一般用梯度下降的方法来拟合线性回归的线，让其尽量的均匀分割散列点。\n\n其中以下是常用的损失函数，用于计算最小化损失。\n\n#### 1. Mean Absolute Error (平均绝对误差)\n\n$$ Error = \\frac{1}{m}\\sum_{i=1}^M|y-\\hat{y}| $$\n\n`缺点: 容易造成收敛缓慢或不收敛`\n\n#### 2. Mean Squared Error (均方误差)\n\n$$ Error = \\frac{1}{2m}\\sum_{i=1}^M(y-\\hat{y})^2 $$\n\n`缺点: 容易受到较大异常值干扰而使斜率偏向较大异常值造成欠拟合`\n\n### 四 . 梯度下降示例代码\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt('data.csv', delimiter=',')  # 加载数据\nX = data[:, :-1]  # 变量\ny = data[:, -1]  # 值\n\n\n# 绘制数据点 用于观察一下大致分布\nplt.scatter(X, y, marker='.')\nplt.show()\n\n# 转换数据集\ndata = np.hstack((np.ones((data.shape[0], 1)), data))\nX_train = data[:, :-1]\ny_train = data[:, -1].reshape((-1, 1))\n\n\ndef hypothesis(X, theta):\n    \"\"\"\n    进行预测\n    \"\"\"\n    return np.dot(X, theta)\n\n\n# function to compute gradient of error function w.r.t. theta\ndef gradient(X, y, theta):\n    \"\"\"\n    计算梯度\n    \"\"\"\n    h = hypothesis(X, theta)\n    grad = np.dot(X.transpose(), (h - y))\n    return grad\n\n\ndef cost(X, y, theta):\n    \"\"\"\n    计算损失函数值\n    \"\"\"\n    h = hypothesis(X, theta)\n    J = 1 / 2 * np.dot((h - y).transpose(), (h - y))\n    return J[0]\n\n\ndef gradient_descent(X, y, learning_rate=0.001, batch_size=25):\n    \"\"\"\n    梯度下降算法\n    \"\"\"\n    history_cost = []\n    theta = np.zeros((X.shape[1], 1))\n    n_points = X.shape[0]\n\n    for _ in range(batch_size):\n        batch = np.random.choice(range(n_points), batch_size)\n\n        X_batch = X[batch, :]\n        y_batch = y[batch]\n\n        theta = theta - learning_rate * gradient(X_batch, y_batch, theta)\n        history_cost.append(cost(X_batch, y_batch, theta))\n\n    return theta, history_cost\n\n\ntheta, error_list = gradient_descent(X_train, y_train, batch_size=1000)\nprint(\"Bias = \", theta[0])\nprint(\"Coefficients = \", theta[1:])\n\n# visualising gradient descent\nplt.plot(error_list)\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\n\ny_pred = hypothesis(X_train, theta)\n#\nplt.scatter(X, y, marker='.')\nplt.plot(X_train[:, 1], y_pred, color='orange')\nplt.show()\n\n```\n\n### 五 . 线性回归示例代码\n\n","tags":["机器学习笔记"]}]