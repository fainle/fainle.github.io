[{"title":"一：链表(linken lists)","url":"/2020/08/13/nd256/数据结构与算法一：链表(linken lists)/","content":"\n### 一 . 简介\n\n链表（Linked list）是一种常见的基础数据结构，是一种线性表，但是并不会按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而顺序表相应的时间复杂度分别是O(logn)和O(1)。\n\n### . python实现\n\n### . 扩展结构\n\n### . 实例应用\n\n\n","tags":["数据结构与算法"]},{"title":"二：设计数据可视化","url":"/2020/08/13/machine learning/Data Visualization/数据可视化二：设计数据可视化/","content":"\n### 一 . 坏的数据可视化\n\n1. 无法传达信息。\n2. 有误导性。 \n\n### 二 . 数据类别\n\n1. Nominal data （无顺序数据，如分类）\n2. Ordinal data （顺序数据，如时间间隔)\n3. Interval data （可加减的数据）\n4. Ratio data （百分比数据）\n\n另外还分为\n\n1. Dicrete (离散 如点)\n2. Continuous （连续性 如线）\n\n还有\n\n1. Likert Scale （评分数据， 如美团点评 的 1-5星）\n\n### 三 . 数据墨水比 （Data-Ink-Ratio）\n\n图形中的数据墨水量除以图形中的总墨水量。\n\n一般比值越高越好\n\n### 四 . 谎言系数 lie factor\n\n$$ \nlifactor = \\frac{（图形最大数据-图形最小数据）/ 图形最小数据}{（真实最大数据 - 真实最小数据） / 真实最小数据}\n$$\n\n其中系数大于1 则图形具有欺骗性\n\n### 五 . 颜色 color\n\n1. 在添加颜色之前，先从黑白开始。\n2. 尽量少使用鲜艳的颜色，为鲜艳的颜色设置灰度值，使颜色更柔和\n3. 颜色要利于区分和传播，比如互补色（如蓝色和红色来区分温度）\n4. 考虑色盲人群，尽量少使用 红绿色\n5. 形状和颜色用于分类\n6. 大小用于数据量的对比\n\n\n\n\n\n","tags":["数据可视化"]},{"title":"一：数据可视化（Data Visualization）简介","url":"/2020/08/13/machine learning/Data Visualization/数据可视化一：简介/","content":"\n### 一 . 概述\n\n数据可视化使复杂的数据更容易理解和使用。\n\n\n### 二 . 步骤\n\n1. Extract （提取数据）\n2. Clean （清洗数据）\n3. Explore （探索数据，做一些简单的可视化尝试）\n4. Analyze （选择一个合适的方法模型）\n5. Share （解释）\n\n### 三 . python常用工具 \n\n1. matplotlib (可视化通过工具)\n2. seaborn （建立在matplotlib的工具，可使常用可视化更容易生成）\n3. pandas （数据处理工具 建立在numpy之上）\n4. numpy （数据计算工具）","tags":["数据可视化"]},{"title":"三：单变量数据探索","url":"/2020/08/13/machine learning/Data Visualization/数据可视化三：单变量数据探索/","content":"\n### 一 . 简介\n\n可视化单一变量，一次只研究一个变量。\n\n### 二 . bar charts\n\n要研究定型变量的分布一般选用柱状图。\n\n1. x 代表分类\n2. y 代表频率（数量）\n3. 一般基线从0开始，以免数据失真\n4. 如果分类没有顺序（如 男女） 则按 y 高低排序比较好\n5. 如果分类有顺序（如 月份），则保持原状\n6. 如果分类很多或名字很长，可以选择90度 用水平柱状图更好。（如编程语言的使用率对比）\n\n### 三 . python 实现\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nimport matplotlib\n\ndf = pd.read_csv('***.csv')\nsns.countplot(data=df, x='status'); # ;好可隐藏图标内存地址\n\nbase_color = sns.color_palette()[0] #获取颜色\ncategory_index = df['category'].value_counts().index #获取排序索引\n\nsns.countplot(data=df, x='category', color=base_color, order=category_index); # x 换成 y 则为水平柱状图\n```","tags":["数据可视化"]},{"title":"十一：高斯混合（GMM）和最大期望（EM）","url":"/2020/08/07/machine learning/Unsupervised Learning/机器学习笔记十一：高斯混合（GMM）和最大期望（EM）/","content":"\n### 一 . 概述\n\n高斯混合模型（Gaussian Mixture Model）：为单一高斯概率密度函数的延伸，用多个高斯概率密度函数（正态分布曲线）精确地量化变量分布，是将变量分布分解为若干基于高斯概率密度函数（正态分布曲线）分布的统计模型。\n\n最大期望算法（Expectation-maximization algorithm，又译期望最大化算法）：在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。\n\n\n### 二 . 算法过程\n\n1. 初始化k个高斯分布\n2. 将数据软聚类成我们初始化的高斯函数（E步骤或期望步骤）\n3. 重新估计高斯（最大化步骤或M步骤）\n4. 估计对数似然来检查收敛\n5. 如果收敛则输出结果，如果不收敛则重复步骤2\n\n\n### 三 . 数学推导\n\n### 四 . sklearn 示例\n\n```python\nfrom sklearn import datasets, mixture\n\nx = datasets.load_iris().data\n\ngmm = mixture.GaussianMixture(n_components=3)\ncluster = gmm.fit_predict(x)\n```","tags":["聚类"]},{"title":"十：密度聚类（Density Based Clustering）","url":"/2020/08/06/machine learning/Unsupervised Learning/机器学习笔记十：密度聚类（Density Based Clustering）/","content":"\n### 一 . 概述\n\nDBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。\n\n\n### 二 . 算法过程\n\n1. 随机找一个点看它周围是否存在其他点(距离由 epsilon)控制, 数量由(minPts)控制。\n2. 如果周围没有任何其他点，则标注为噪音\n3. 如果周围有足够数量(minPts)的点,则标注为核心点\n4. 被核心点辐射且本身不是核心的点为边界点\n\n\n### 三 . 优缺点\n\n#### 1. 优点\n\n1. 不需要指明类的数量\n2. 灵活的分离各种形状和大小的类\n3. 能强力的处理噪音和离群值\n\n\n#### 2. 缺点\n\n1. 不同密度的类，分类比较困难(HDBSCAN可解决)\n\n### 四 . sklearn 示例\n\n```python\nfrom sklearn import datasets, cluster\n\nx = datasets.load_iris().data\n\ndbscan = cluster.DBSCAN(eps=0.5, min_samples=5)\n\ndbscan.fit(x)\ndbscan.labels_\n```","tags":["聚类"]},{"title":"九：层次聚类（Hierarchical clustering）","url":"/2020/08/06/machine learning/Unsupervised Learning/机器学习笔记九：层次聚类（Hierarchical clustering）/","content":"\n### 一 . 概述\n\n层次聚类是一种聚类分析方法，旨在构建聚类的层次结构。\n对于层次聚类策略一般分为两种类型： \n\n1. 自下而上：\n\n每个观测值都从其自己的聚类开始，随着层次结构的上升，聚类对合并在一起。\n\n2. 自上而下：\n\n所有观测都从一个群集开始，并且随着层次向下移动，拆分将递归执行。\n\n\n### 二 . 算法分类\n\n#### 1. SingleLinkage\n\n取两个类中最近的两个样本之间的距离作为两个集合的距离，即：最近的两个样本之间的距离越小，\n\n#### 2. CompleteLinkage\n\n取两个集合距离最远的两个点的距离作为两个集合的距离，其效果也刚好相反，限制非常大。\n\n#### 3. AverageLinkage\n\n两个集合中的点两两距离全部放在一起求平均值，相应的能得到一点合适的结果。\n\n### 三 . 优缺点\n\n#### 1. 优点\n\n1. 层次表达的结构信息丰富\n2. 数据容易可视化\n\n\n#### 2. 缺点\n\n1. 对噪音和离群值很敏感\n2. 计算量很大\n\n### 四 . sklearn 示例\n\n```python\nfrom sklearn import datasets, cluster\nfrom scipy.cluster.hierarchy import dendrogram, ward, single\nimport matplotlib.pyplot as plt\n\nx = datasets.load_iris().data[:10]\nclust = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward')  # ward = 离平方差\nlabels = clust.fit_predict(x)\n\nlinkage_matrix = ward(x)\ndendrogram(linkage_matrix)\nplt.show()\n```","tags":["聚类"]},{"title":"八：K-平均算法 (K-Means)","url":"/2020/08/05/machine learning/Unsupervised Learning/机器学习笔记八：k-means/","content":"\n### 一 . 概述\n\nK-Means算法用于对各种数据进行聚类。\n\n其中 k 代表具有的族类数\n\n1. 相似类型的书籍或作者\n2. 类似的电影\n3. 类似的音乐\n4. 类似兴趣的客户\n\n### 二 . 工作原理\n\n1. 随机设置每个K分类点\n2. 将每个点分给每个最近的K分类点\n3. 移动K到分类的点的中心区域\n\n### 三 . sklearn 示例\n\n```python\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nfrom IPython import display\n\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = [16, 9]\n\n# 生成聚族随机数据 其中1000个点 10个特征，10个中心点\ndata, y = make_blobs(n_samples=1000, n_features=10, centers=10, random_state=42)\n\n# 预测最佳的点\nsocres = []\nfor i in range(1, 12):\n    kmeans = KMeans(i)\n    model = kmeans.fit(data)\n    socre = model.score(data)\n    socres.append(abs(socre))\n    \nx_plot_data = list(range(1, 12))\nplt.plot(x_plot_data, socres)\n\n# 3D可视化\nkmeans2 = KMeans(10)\nmodel2 = kmeans2.fit(data)\nlabels = model2.predict(data)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(data[:, 0], data[:, 1], data[:, 2], c=labels, cmap='tab10')\n```","tags":["聚类"]},{"title":"八：模型训练（Training）","url":"/2020/07/30/machine learning/Neural Networks/神经网络学习笔记：八 模型训练（Training)/","content":"\n### 一 . 正则化\n\n#### 1 . L1\n#### 2 . L2\n\n### 二 . Dropout\n\n训练的时候随机关闭网络中的节点\n\n### 三 . 局部最小值 (Local Minima)\n\n### 四 . 随机梯度下降 \n\n### 五 . S函数\n\n### 六 . 学习率（Learning)\n","tags":["深度学习笔记"]},{"title":"七：反向传播（Backpropagation）","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：七 反向传播（Gradient Descent）/","content":"\n### 1 . 概述\n\n反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。\n\n反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。\n\n```python\ndef sigmoid(x):\n    return 1 / 1-np.exp(-x)\ndef sigmoid_prime(x):\n    return sigmoid(x) * (1-sigmoid(x))\ndef error_formula(y, output):\n    return - y*np.log(output) - (1 - y) * np.log(1-output)\ndef error_term_formula(y, output):\n    return (y-output) * output * (1 - output)\n```","tags":["深度学习笔记"]},{"title":"六：梯度下降（Gradient Descent）","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：六 梯度下降（Gradient Descent）/","content":"\n### 1 . 概述\n\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，主要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长（学习率）距离点进行迭代搜索。\n\n### 2 . SSE\n\n$$ E = \\frac{1}{2}\\sum_\\mu(y^{\\mu}-\\hat{y}^{\\mu})^2 $$\n\n$$ \\hat{y} = \\sigma(\\sum_{i}w_{i}x_{i}^{u}) $$\n\n$$ E = \\frac{1}{2}\\sum_\\mu(y^{\\mu}- \\sigma(\\sum_{i}w_{i}x_{i}^{u}) )^2 $$\n\n### 3 . 代码实现\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_points(X, y):\n    admitted = X[np.argwhere(y==1)]\n    rejected = X[np.argwhere(y==0)]\n    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n\ndef display(m, b, color='g--'):\n    plt.xlim(-0.05,1.05)\n    plt.ylim(-0.05,1.05)\n    x = np.arange(-10, 10, 0.1)\n    plt.plot(x, m*x+b, color)\n    \n    \ndata = pd.read_csv('data.csv', header=None)\nx = np.array(data[[0,1]])\ny = np.array(data[2])\nplot_points(x,y)\nplt.show()\n\n\ndef sigmoid(x):\n    \"\"\"\n    sigmoid 计算\n    s(x) = 1/ (1 + e^-x)\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef output_formula(features, weights, bias):\n    \"\"\"\n    拟合函数\n    x : 特征\n    w : 权重\n    b : 偏移\n    y = s(Wx) + b\n    \"\"\"\n    return sigmoid(np.dot(features, weights) + bias)\n\ndef error_formula(y, output):\n    \"\"\"\n    错误率计算公式\n    y : 预测值\n    f : output_formula\n    error = -ylog(f)-(1-y)log(1-f)\n    \"\"\"\n    return np.dot(-y, np.log(output)) - np.dot((1 - y), np.log(1-output))\n#     return -y * np.log(f) - (1 - y) * np.log(1 - f)\n\ndef update_weights(x, y, weights, bias, learnrate=0.1):\n    \"\"\"\n    更新权重\n    \"\"\"\n    output = output_formula(x, weights, bias)\n    d_error = -(y - output)\n    weights -= learnrate * d_error * x\n    bias -= learnrate * d_error\n    return weights, bias\n\ndef train(features, targets, epochs, learnrate, graph_lines=False):\n    \n    errors = []\n    n_records, n_features = features.shape\n    last_loss = None\n    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n    bias = 0\n    for e in range(epochs):\n        del_w = np.zeros(weights.shape)\n        for x, y in zip(features, targets):\n            output = output_formula(x, weights, bias)\n            error = error_formula(y, output)\n            weights, bias = update_weights(x, y, weights, bias, learnrate)\n        \n        # Printing out the log-loss error on the training set\n        out = output_formula(features, weights, bias)\n        loss = np.mean(error_formula(targets, out))\n        errors.append(loss)\n        if e % (epochs / 10) == 0:\n            print(\"\\n========== Epoch\", e,\"==========\")\n            if last_loss and last_loss < loss:\n                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n            else:\n                print(\"Train loss: \", loss)\n            last_loss = loss\n            predictions = out > 0.5\n            accuracy = np.mean(predictions == targets)\n            print(\"Accuracy: \", accuracy)\n        if graph_lines and e % (epochs / 100) == 0:\n            display(-weights[0]/weights[1], -bias/weights[1])\n            \n\n    # Plotting the solution boundary\n    plt.title(\"Solution boundary\")\n    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n\n    # Plotting the data\n    plot_points(features, targets)\n    plt.show()\n\n    # Plotting the error\n    plt.title(\"Error Plot\")\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Error')\n    plt.plot(errors)\n    plt.show()\n```","tags":["深度学习笔记"]},{"title":"五：逻辑回归（Logistic Regression）","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：五 逻辑回归（Logistic Regression）/","content":"\n### 1 . 计算方式\n\n1 如果 y = 1\n\n$$ P(1) = \\hat{y} $$\n\n错误为\n\n$$ error = -log(\\hat{y}) $$\n\n2 如果 y = 0\n\n$$ P(0) = 1 - \\hat{y} $$\n\n错误为\n\n$$ error = -log(1-\\hat{y}) $$\n\n错误为\n\n$$ error = -(1-y)(\\log(1-\\hat{y})) - y\\log(\\hat{y}) $$\n\n损失函数为\n\n$$ \nError Func = -\\frac{1}{m}\\sum_{i=1}^m(1-y_i)(\\log(1-\\hat{y}_{i}) + y_{i}\\log(\\hat{y}_i)\n$$\n\n其中 \n\n$$ \\hat{y} = \\sigma(Wx^{(i)} + b) $$\n\n最终得到\n\n$$ \nE(W, b) = -\\frac{1}{m}\\sum_{i=1}^m(1- y_i )(\\log(1- \\sigma(Wx^{(i)} + b) ) + y_{i}\\log( \\sigma(Wx^{(i)} + b) )\n$$\n\n### 1 . 计算步骤\n\n1 随机w1...wn 和 b 得到一条线段（函数）\n\n2 对每个点按照学习率 计算得到新的 w1...wn 和 b \n\n3 重复过程让 错误率最小 \n\n","tags":["深度学习笔记"]},{"title":"四：最大似然（Maximum Likelihood）","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：四 最大似然（Maximum Likelihood）/","content":"\n### 一 . 概述\n\n最大似然估计（英语：maximum likelihood estimation，缩写为MLE），也称极大似然估计、最大概似估计，是用来估计一个概率模型的参数的一种方法。\n\n### 二 . 交叉熵（cross entropy)\n\n$$\nce = -\\sum^m_{i=1}\\ y_ilog(p_i) + (1-y_i)log(1-p_i)\n$$\n\n例子\n```\np1 = 0.8 p2 = 0.7 p3 = 0.1\n\nCE[(1, 1, 0), (0.8, 0.7, 0.1)] = - (ln(0.8) + ln(0.7) + ln(0.9)) = 0.68\n\nCE[(0, 0, 1), (0.8, 0.7, 0.1)] = - (ln(0.2) + ln(0.3) + ln(0.1)) = 5.12\n```\n\n### 三 . 交叉熵代码示例\n```python\nimport numpy as np\n\ndef cross_entropy(Y, P):\n    Y = np.float_(Y)\n    P = np.float_(P)\n    \n    return -np.sum(Y * np.log(P) + (1-Y) * np.log(1-P) )\n\ncross_entropy(np.array([1, 1, 0]), np.array([0.8, 0.7, 0.1]))\n```\n\n\n### 四 . 多分类交叉熵（Multi-Class Cross Entropy)\n\n$$\nce = -\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}\\log(p_{ij})\n$$","tags":["深度学习笔记"]},{"title":"三：Softmax 函数","url":"/2020/07/28/machine learning/Neural Networks/神经网络学习笔记：三 Softmax 函数/","content":"\n### 一 . 概述\n\n它能将一个含任意实数的K维向量 `z` “压缩”到另一个K维实向量 $$ \\sigma (z) $$中，使得每一个元素的范围都在(0, 1)之间，并且所有元素的和为1(也可视为一个 (k-1)维的hyperplan，因为总和为1，所以是subspace)。\n\n$$ \n\\sigma(z)_{j} = \\frac{e^{zj}}{\\sum_{k=1}^{K}e^{z}k} \\quad for\\ j = 1 ... k\n$$\n\n\n\n### 二 . 实现代码\n\n```python\nimport numpy as np\n\nl = np.array([-1, 0, 1])\n\ndef softmax(l):\n    exp = np.exp(l)\n    return np.divide(exp, exp.sum())\n\nsoftmax(l)\n```","tags":["深度学习笔记"]},{"title":"二：Sigmoid 函数","url":"/2020/07/27/machine learning/Neural Networks/神经网络学习笔记：二 sigmoid 函数/","content":"\n### 一 . 概述\n\nSigmoid函数得名因其形状像S字母。一种常见的S函数是逻辑函数\n\n$$ \nS(t) = \\frac{1}{1 + e^{-t}}\n$$\n\n优点：平滑、易于求导。\n\n缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。\n","tags":["深度学习笔记"]},{"title":"一：感知器 (Perceptrons)","url":"/2020/07/27/machine learning/Neural Networks/神经网络学习笔记：一 感知器/","content":"\n### 一 . 概述\n\n感知器是一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器。\n\n### 二 . 定义\n\n感知器使用特征向量来表示的前馈神经网络。\n\n$$ \nf(x) = \n\\begin{cases} \n1\\quad if w.x + b > 0 \\\\ \n0\\quad else\n\\end{cases} \n\n$$\n\nw是实数的表示权重的向量，`w.x`是点积。`b`是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级。\n\n### 三 .作为逻辑运算符的感知器 \n\n这种感知器 可以执行 `and` `or` 和 `not` 运算。\n\n这种感知器有2个输入值为{true, false} 类似 {1, 0} 和 一个输出 `yes` or `no`.\n\n#### 1 . AND 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  yes  |\n| 1  | 0 |  no  |\n| 0  | 1 |  no  |\n| 0  | 0 |  no  |\n\n#### 2 . OR 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  yes  |\n| 1  | 0 |  yes  |\n| 0  | 1 |  yes  |\n| 0  | 0 |  no  |\n\n#### 3 . NOT 感知器\n\n|   IN   |  OUT |\n|  ----  |  ---- |\n| 1  |  yes  |\n| 0   |  no  |\n\n#### 4 . XOR 感知器\n\n|   IN  | IN  |  OUT |\n|  ----  | ----  |  ---- |\n| 1  | 1 |  no  |\n| 1  | 0 |  yes  |\n| 0  | 1 |  yes |\n| 0  | 0 |  no |\n\n\n### 四 . 感知器算法\n\n```\nimport pandas as pd\n\nnp.random.seed(42)\n\ndef stepFunction(t):\n    if t >= 0:\n        return 1\n    return 0\n\n## 设置函数\ndef prediction(X, W, b):\n    return stepFunction((np.matmul(X,W)+b)[0])\n\n## 计算新权重\ndef perceptronStep(X, y, W, b, learn_rate = 0.01):\n    for i in range(len(X)):\n        y_hat = prediction(X[i],W,b)\n        \n        if y[i] - y_hat == 1:\n            W[0] += X[i][0]*learn_rate\n            W[1] += X[i][1]*learn_rate\n        elif y[i] - y_hat == -1:\n            W[0] -= X[i][0]*learn_rate\n            W[1] -= X[i][1]*learn_rate\n    \n    return W, b\n    \n## 感知器算法\ndef trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n    x_min, x_max = min(X.T[0]), max(X.T[0])\n    y_min, y_max = min(X.T[1]), max(X.T[1])\n    W = np.array(np.random.rand(2,1))\n    b = np.random.rand(1)[0] + x_max\n    # These are the solution lines that get plotted below.\n    boundary_lines = []\n    for i in range(num_epochs):\n        # In each epoch, we apply the perceptron step.\n        W, b = perceptronStep(X, y, W, b, learn_rate)\n        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n    return boundary_lines\n\n\ndata = np.asarray(pd.read_csv('data.csv'))\nx = data[:, 0:data.shape[1]-1]\ny = data[:, -1]\n\nres = trainPerceptronAlgorithm(x, y)\n```\n\n","tags":["深度学习笔记"]},{"title":"七：独热编码 (One-Hot Encoding)","url":"/2020/07/27/machine learning/机器学习笔记七：文本独热编码/","content":"\n### 一 . 概述\n\n使用pandas对Dataframe上进行One-Hot Encoding的例子。\n\n### 二 . Pandas Dataframe 中的热编码。\n\n首先我们创建一个DataFrame数据集。\n\n```\nimport pandas as pd\n\ndata = pd.DataFrame({'color': ['blue', 'red', 'yellow']})\n```\n\n![](/img/1.png)\n\npandas上有一个`get_dummies`方法可以对DataFrame数据，实现我们想要的独热编码功能。\n\n```\npd.get_dummies(data['color'], prefix='color')  ## 其中 prefix 为新的特征名称前缀\n```\n\n![](/img/2.png)\n\n有时候我们可能会出现在预测数据集合里面存在 在训练集合里面没有的特质数据。比如 `green`，这样会让独热编码出错。这种情况可以使用CategoricalDType方法来处理\n\n```\ndf['color'] = df['color'].astype(pd.CategoricalDtype(['blue', 'green']))\n```\n\n这样其它异常值会被忽略\n\n使用pd.concat可以将新的数据列和原始数据链接起来\n\n```\ndf = pd.concat([df, pd.get_dummies(df['color'], prefix='color')], axis=1)\n```\n\n也可以使用 `dummy_na=True` 参数保留 `NaN` 值为一个单独的独热编码\n\n```\ndf = pd.concat([df, pd.get_dummies(df['color'], prefix='color', dummy_na=True)], axis=1)\n```\n\n最后结构如下图\n\n![](/img/3.png)\n\n","tags":["pandas"]},{"title":"六：模型评估 (Model Evaluation Metrics)","url":"/2020/07/25/machine learning/机器学习笔记六：模型评估/","content":"\n### 一 . 概述\n\n模型评估是通过一系列评估方法评估模型的好坏。\n\n### 二 . Confusion Matrix (混淆矩阵)\n\n在机器学习领域和统计分类问题中，混淆矩阵是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。矩阵的每一列代表一个类的实例预测，而每一行表示一个实际的类的实例。之所以如此命名，是因为通过这个矩阵可以方便地看出机器是否将两个不同的类混淆了。\n\n|     | 真  |  假 |\n|  ----  | ----  |  ---- |\n| 真  | 真真 = true |  真假 = false  |\n| 假  | 假真 = false |  假假 = true  |\n\n### 三 . Accuracy (准确度)\n\n$$ acc = \\frac{真真 + 假假}{总数} $$ \n\n\n### 四 . Precision （准确率)\n\n$$ precision = \\frac{真真}{真真 + 假真} $$ \n\n查得更准\n\n### 五 . Recall （召回率)\n\n$$ recall = \\frac{真真}{真真 + 真假 } $$ \n\n查得更全\n\n一般情况下 准确率 和 召回率 越高越好，但真实情况需要根据业务的情况。\n\n### 六 . F1 Score \n\n$$ f1 = 2\\frac{precision * recall}{precision + recall } $$ \n\n更偏向于那个指标。\n\n### 七 . ROC \n\nroc 越大模型越好\n\n### 八 . sklearn 例子\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve\nimport numpy as np\n\ndata = np.asarray(pd.read_csv('data.csv'))\nx = data[:, 0:2]\ny = data[:, 2]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nprecision_score = precision_score(y_test, y_pred)\nrecall_score = recall_score(y_test, y_pred)\nf1_score = f1_score(y_test, y_pred)\nroc_curve = roc_curve(y_test, y_pred)\n\nprint('acc: ', acc)\nprint('precision_score: ', precision_score)\nprint('recall_score: ', recall_score)\nprint('f1_score: ', f1_score)\nprint('roc_curve: ', roc_curve)\n```\n","tags":["统计机器学习笔记"]},{"title":"五：集成方法 (Ensemble Methods)","url":"/2020/07/25/machine learning/机器学习笔记五：集成方法/","content":"\n### 一 . 概述\n\n集成方法是将多个模型组合起来获得更好的模型的一种方法。一般分为 bagging 和 boosting。\n\n### 二 . 所属分类\n\n集成方法属于 监督学习。\n  \n### 三 . bagging\n\n装袋算法, 对多个模型的结果投票，平均等方式获得结果。\n\n代表算法:\n\n#### 1 . 随机森林 (Random Forests)\n\n随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。\n\n### 四 . boosting。\n\n提升方法\n\n多个弱分类器组合成一个强分类器的算法，通常给不同的弱分类器不同的权重。数据会被重新加权。\n\n代表算法:\n\n#### 1 . AdaBoost\n\n自适应增强\n\nAdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。\n\n其中权重计算公式 \n\n$$ weight = ln(\\frac{accuracy}{1-accuracy}) $$\n\n例子\n```\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), n_estimators = 4) # base_estimator 选择的弱模型，n_estimators 模型的数量\n\nmodel.fit(x_train, y_train)\nmodel.predict(x_test)\n```","tags":["统计机器学习笔记"]},{"title":"四：支持向量机 (Support Vector Machine)","url":"/2020/07/24/machine learning/机器学习笔记四：支持向量机/","content":"\n### 一 . 概述\n\n支持向量机是一种分类算法，它可以找到潜在的最佳分割线。\n\n### 二 . 所属分类\n\n支持向量机属于 监督学习。\n  \n### 三 . Kernels\n\n#### 1 . polynomial\n\n#### 2 . rbf\n\n### 四 . sklearn 支持向量机示例\n\n```\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\n\ndata = np.asarray(pd.read_csv('data.csv', header=None))\n\nX = data[:,0:2]\ny = data[:,2]\n\nmodel = SVC(kernel='rbf', gamma=27)\n\nmodel.fit(X,y)\n\ny_pred = model.predict(X)\n\nacc = accuracy_score(y, y_pred)\n```","tags":["统计机器学习笔记"]},{"title":"一：朴素贝叶斯垃圾邮件分类","url":"/2020/07/24/machine learning/机器学习练习项目一：垃圾邮件分类/","content":"\n### 一 . 概述\n\n使用朴素贝叶斯算法实现垃圾邮件分类。\n\n###二 . 数据集\n\n[kaggle提供的数据集](https://www.kaggle.com/venky73/spam-mails-dataset)\n  \n### 三 . 代码\n\n```python\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer  # td-if\nfrom sklearn.model_selection import train_test_split # 用于切割数据\nfrom sklearn.naive_bayes import MultinomialNB # 朴素贝叶斯模型\n\ndata = pd.read_csv('spam_ham_dataset.csv') # 载入数据\n\nx = data[['text']] # 特征\ny = data[['label']] # 值\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # 划分训练数据和测试数据 比例 1:4\n\nprint('train data number: {d}'.format(d=len(x_train)))\nprint('test data number: {d}'.format(d=len(x_test)))\n\ncv = CountVectorizer()\ncv.fit(x['text'])\ncount = cv.transform(x_train['text'])\n\ntfidf = TfidfTransformer()\ntfidf.fit(count)\ntfidf_matrix = tfidf.transform(count)\n\nmodel = MultinomialNB()\nmodel.fit(tfidf_matrix, y_train)\n\nmodel.predict(tfidf.transform(cv.transform(x_test['text'])))\n\nmodel.score(tfidf.transform(cv.transform(x_test['text'])), y_test)\n\n```\n","tags":["机器学习练习项目"]},{"title":"三：朴素贝叶斯 (Naive Bayes)","url":"/2020/07/23/machine learning/机器学习笔记三：朴素贝叶斯/","content":"\n### 一 . 概述\n\n朴素贝叶斯是一种概率算法，基于条件概率这样概念。它具有容易实现，训练速度很快等优点。\n\n###二 . 所属分类\n\n朴素贝叶斯属于 监督学习。\n  \n### 三 . 贝叶斯推理公式\n\n$$ P(A|R) = \\frac{P(A)P(R|A)}{P(A)P(R|A) + P(B)P(R|B)} $$\n\n`其中 P(A) P(B) 为先验概率`\n\n### 四 . 练习题\n\n已知一种传染病的感染概率为万分之一，医生检查准确率为99%，如果一个病人结果为阳性，请问他得这种传染病的概率是多少？\n\nA = 感染 B = 健康 R = 后验概率\n\n答案\n\n>P(A) = 0.0001 (得病概率)\n> P(B) = 0.9999 (健康概率)\n> P(R|A) = 0.99 (后验得病概率)\n> p(R|B) = 0.01 (后验健康概率)\n> P(A|R) = 0.0001 * 0.99 / (0.0001 * 0.99 + 0.9999 * 0.01) = 0.0098\n\n### 五 . 贝叶斯算法\n\n#### 1 . Gaussian Naive Bayes\n#### 2 . Multinomial Naive Bayes\n#### 3 . Complement Naive Bayes\n#### 4 . Bernoulli Naive Bayes\n#### 5 . Categorical Naive Bayes\n#### 6 . Out-of-core naive Bayes model fitting\n\n### 六 . sklearn 多项式朴素贝叶斯（Multinomial Naive Bayes）示例\n\n```python\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\n\nrng = np.random.RandomState(1)\n\nX = rng.randint(10, size=(2, 100))\ny = np.array([1, 2])\n\nclf.fit(X, y)\n\nyred = clf.predict(X[0:1])\n```","tags":["统计机器学习笔记"]},{"title":"二：决策树 (Decision Trees)","url":"/2020/07/23/machine learning/机器学习笔记二：决策树/","content":"\n### 一 . 概述\n\n决策树是一个预测模型；它代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，可以用于分析数据，同样也可以用来作预测。\n\n###二 . 所属分类\n\n决策树一般属于 监督学习。\n  \n### 三 . Entropy (熵)\n\n决策树需要用到熵。\n\n熵是一个物理学概念，是一种测量在动力学方面不能做功的能量总数，也就是当总体的熵增加，其做功能力也下降，熵的量度正是能量退化的指标。熵亦被用于计算一个系统中的失序现象，也就是计算该系统混乱的程度。熵越低越稳定。\n\n公式\n\n$$ Entropy = -p_1log_2(p_1) - p_2log_2(p_2) ... -p_nlog_2(p_n) $$\n\n简化\n$$ Entropy = -\\sum_{n=1}^np_nlog_2(p_n) $$\n\n其中p \n$$ p_i = \\frac{p_i}{n} $$ \n\n\n### 四 . Information Gain (信息增益)\n\n信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度。\n\n信息增益是特征选择的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，相应的信息增益也就越大。\n\n$$ Gain(D) = Entropy(D) - \\frac{D^v}{D}\\sum_{v=1}^vEntropy(D^v) $$\n\n### 五 . sklearn 决策树示例\n\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata = np.asarray(pd.read_csv('data.csv', header=None)) # 读取数据 去掉header头 并转为numpy array\n\nX = data[:, 0:2]\ny= data[:, 2]\n\nmodel = DecisionTreeClassifier(max_depth=7, min_samples_leaf=1, ) # 选择决策树模型 其中最大深度为7 最小样本数为10 （当前数据中 如最小样本数为1 则可以准确性达到 100% , 最小样本数决定是否剪枝)\n\nreg = model.fit(X,y)  # 训练模型\ny_pred = model.predict(X)  #预测模型\n\nacc = accuracy_score(y, y_pred)  # 查看计算准确度\n```\n\n### 六 . 可视化\n```\nimport graphviz\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(model, out_file='tree.dot',\n#                                 feature_names=data.columns[:-1], # 特征名称\n                                class_names=['No', 'Yes'], # 目标变量的类别名\n                                filled=True, rounded=True,\n                                special_characters=True)\n\nwith open('tree.dot') as f:\n    dot = f.read()\n    \ngraphviz.Source(dot)\n```\n\n","tags":["统计机器学习笔记"]},{"title":"一：线性回归 (Linear Regression)","url":"/2020/07/21/machine learning/机器学习笔记一：线性回归/","content":"\n### 一 . 概述\n在统计学中，线性回归是利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系的一种分析。\n\n一个自变量称为一元回归。\n\n多个自变量称为多元回归。\n\n###二 . 所属分类\n\n线性回归一般属于 监督学习。\n  \n### 三 . 数学相关\n\n一般用梯度下降的方法来拟合线性回归的线，让其尽量的均匀分割散列点。\n\n其中以下是常用的损失函数，用于计算最小化损失。\n\n#### 1. Mean Absolute Error (平均绝对误差)\n\n$$ Error = \\frac{1}{m}\\sum_{i=1}^M|y-\\hat{y}| $$\n\n`缺点: 容易造成收敛缓慢或不收敛`\n\n#### 2. Mean Squared Error (均方误差)\n\n$$ Error = \\frac{1}{2m}\\sum_{i=1}^M(y-\\hat{y})^2 $$\n\n`缺点: 容易受到较大异常值干扰而使斜率偏向较大异常值造成欠拟合`\n\n### 四 . 梯度下降示例代码\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt('data.csv', delimiter=',')  # 加载数据\nX = data[:, :-1]  # 变量\ny = data[:, -1]  # 值\n\n\n# 绘制数据点 用于观察一下大致分布\nplt.scatter(X, y, marker='.')\nplt.show()\n\n# 转换数据集\ndata = np.hstack((np.ones((data.shape[0], 1)), data))\nX_train = data[:, :-1]\ny_train = data[:, -1].reshape((-1, 1))\n\n\ndef hypothesis(X, theta):\n    \"\"\"\n    进行预测\n    \"\"\"\n    return np.dot(X, theta)\n\n\n# function to compute gradient of error function w.r.t. theta\ndef gradient(X, y, theta):\n    \"\"\"\n    计算梯度\n    \"\"\"\n    h = hypothesis(X, theta)\n    grad = np.dot(X.transpose(), (h - y))\n    return grad\n\n\ndef cost(X, y, theta):\n    \"\"\"\n    计算损失函数值\n    \"\"\"\n    h = hypothesis(X, theta)\n    J = 1 / 2 * np.dot((h - y).transpose(), (h - y))\n    return J[0]\n\n\ndef gradient_descent(X, y, learning_rate=0.001, batch_size=25):\n    \"\"\"\n    梯度下降算法\n    \"\"\"\n    history_cost = []\n    theta = np.zeros((X.shape[1], 1))\n    n_points = X.shape[0]\n\n    for _ in range(batch_size):\n        batch = np.random.choice(range(n_points), batch_size)\n\n        X_batch = X[batch, :]\n        y_batch = y[batch]\n\n        theta = theta - learning_rate * gradient(X_batch, y_batch, theta)\n        history_cost.append(cost(X_batch, y_batch, theta))\n\n    return theta, history_cost\n\n\ntheta, error_list = gradient_descent(X_train, y_train, batch_size=1000)\nprint(\"Bias = \", theta[0])\nprint(\"Coefficients = \", theta[1:])\n\n# visualising gradient descent\nplt.plot(error_list)\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\n\ny_pred = hypothesis(X_train, theta)\n#\nplt.scatter(X, y, marker='.')\nplt.plot(X_train[:, 1], y_pred, color='orange')\nplt.show()\n\n```\n\n### 五 . 线性回归示例代码 (sklearn)\n\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv('data.csv', delimiter = ',')  # 加载数据\n\nX = data.iloc[:,0].to_frame()\ny = data.iloc[:,1].to_frame()\n\nlr_model = LinearRegression()\nreg = lr_model.fit(X, y)\ny_pred = reg.predict(X)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='orange')\nplt.show()\n```\n\n### 六 . 多元线性回归\n\n多元线性回归基本特征和单元一直，但多元线性回归无法可视化。\n\n多元线性回归例子\n\n```\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\n\nmodel = LinearRegression()\n\nreg = model.fit(x, y)\n\nsample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n\nreg.predict(sample_house)\n```\n\n### 一些问题\n\n1 数据分布非线形的时候 线性回归不理想\n\n2 数据异常值较大时 会引起欠拟合\n\n\n### 多项式回归\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_data = pd.read_csv('data1.csv')\nX = train_data['Var_X'].values.reshape(-1, 1)\ny = train_data['Var_Y'].values\n\npoly_feat = PolynomialFeatures(degree = 2)\nX_poly = poly_feat.fit_transform(X)\n\npoly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)\n\ny_pred = poly_model.predict(X_poly)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='blue')\nplt.show()\n```\n\n\n","tags":["统计机器学习笔记"]}]