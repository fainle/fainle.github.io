[{"title":"一：线性回归","url":"/2020/07/21/机器学习笔记一：线性回归/","content":"\n### 一 . 概述\n在统计学中，线性回归是利用线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系的一种分析。\n\n一个自变量称为一元回归。\n\n多个自变量称为多元回归。\n\n###二 . 所属分类\n\n线性回归一般属于 监督学习。\n  \n### 三 . 数学相关\n\n一般用梯度下降的方法来拟合线性回归的线，让其尽量的均匀分割散列点。\n\n其中以下是常用的损失函数，用于计算最小化损失。\n\n#### 1. Mean Absolute Error (平均绝对误差)\n\n$$ Error = \\frac{1}{m}\\sum_{i=1}^M|y-\\hat{y}| $$\n\n`缺点: 容易造成收敛缓慢或不收敛`\n\n#### 2. Mean Squared Error (均方误差)\n\n$$ Error = \\frac{1}{2m}\\sum_{i=1}^M(y-\\hat{y})^2 $$\n\n`缺点: 容易受到较大异常值干扰而使斜率偏向较大异常值造成欠拟合`\n\n### 四 . 梯度下降示例代码\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt('data.csv', delimiter=',')  # 加载数据\nX = data[:, :-1]  # 变量\ny = data[:, -1]  # 值\n\n\n# 绘制数据点 用于观察一下大致分布\nplt.scatter(X, y, marker='.')\nplt.show()\n\n# 转换数据集\ndata = np.hstack((np.ones((data.shape[0], 1)), data))\nX_train = data[:, :-1]\ny_train = data[:, -1].reshape((-1, 1))\n\n\ndef hypothesis(X, theta):\n    \"\"\"\n    进行预测\n    \"\"\"\n    return np.dot(X, theta)\n\n\n# function to compute gradient of error function w.r.t. theta\ndef gradient(X, y, theta):\n    \"\"\"\n    计算梯度\n    \"\"\"\n    h = hypothesis(X, theta)\n    grad = np.dot(X.transpose(), (h - y))\n    return grad\n\n\ndef cost(X, y, theta):\n    \"\"\"\n    计算损失函数值\n    \"\"\"\n    h = hypothesis(X, theta)\n    J = 1 / 2 * np.dot((h - y).transpose(), (h - y))\n    return J[0]\n\n\ndef gradient_descent(X, y, learning_rate=0.001, batch_size=25):\n    \"\"\"\n    梯度下降算法\n    \"\"\"\n    history_cost = []\n    theta = np.zeros((X.shape[1], 1))\n    n_points = X.shape[0]\n\n    for _ in range(batch_size):\n        batch = np.random.choice(range(n_points), batch_size)\n\n        X_batch = X[batch, :]\n        y_batch = y[batch]\n\n        theta = theta - learning_rate * gradient(X_batch, y_batch, theta)\n        history_cost.append(cost(X_batch, y_batch, theta))\n\n    return theta, history_cost\n\n\ntheta, error_list = gradient_descent(X_train, y_train, batch_size=1000)\nprint(\"Bias = \", theta[0])\nprint(\"Coefficients = \", theta[1:])\n\n# visualising gradient descent\nplt.plot(error_list)\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"Cost\")\nplt.show()\n\ny_pred = hypothesis(X_train, theta)\n#\nplt.scatter(X, y, marker='.')\nplt.plot(X_train[:, 1], y_pred, color='orange')\nplt.show()\n\n```\n\n### 五 . 线性回归示例代码 (sklearn)\n\n```\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv('data.csv', delimiter = ',')  # 加载数据\n\nX = data.iloc[:,0].to_frame()\ny = data.iloc[:,1].to_frame()\n\nlr_model = LinearRegression()\nreg = lr_model.fit(X, y)\ny_pred = reg.predict(X)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='orange')\nplt.show()\n```\n\n### 六 . 多元线性回归\n\n多元线性回归基本特征和单元一直，但多元线性回归无法可视化。\n\n多元线性回归例子\n\n```\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\n\nboston_data = load_boston()\nx = boston_data['data']\ny = boston_data['target']\n\nmodel = LinearRegression()\n\nreg = model.fit(x, y)\n\nsample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n\nreg.predict(sample_house)\n```\n\n### 一些问题\n\n1 数据分布非线形的时候 线性回归不理想\n\n2 数据异常值较大时 会引起欠拟合\n\n\n### 多项式回归\n\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ntrain_data = pd.read_csv('data1.csv')\nX = train_data['Var_X'].values.reshape(-1, 1)\ny = train_data['Var_Y'].values\n\npoly_feat = PolynomialFeatures(degree = 2)\nX_poly = poly_feat.fit_transform(X)\n\npoly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)\n\ny_pred = poly_model.predict(X_poly)\n\nplt.scatter(X, y, marker='.')\nplt.plot(X, y_pred, color='blue')\nplt.show()\n```\n\n\n","tags":["机器学习笔记"]}]